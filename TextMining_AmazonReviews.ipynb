{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQK54T76x9h4"
      },
      "source": [
        "## 2. Criação de uma baseline\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tbC3PDdnyJV6"
      },
      "source": [
        "### Vader Sentiment\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zRY6XUzAwp9A",
        "outputId": "b324c2b1-d3a8-4e94-edf4-2f5becb3e0e4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n"
          ]
        }
      ],
      "source": [
        "pip install nltk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zVSJ4NS69AF0"
      },
      "source": [
        "Import the necessary libraries and download the VADER lexicon."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AQrth5IQ8vcb",
        "outputId": "d1084410-ef6d-46f4-fd0b-84cbfebc031e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "\n",
        "# Download the VADER lexicon (if you haven't already)\n",
        "nltk.download('vader_lexicon')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1LaWG0vc9b9A"
      },
      "source": [
        "Create a SentimentIntensityAnalyzer object and use it to analyze text sentiment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "sLjJJWT585X9"
      },
      "outputs": [],
      "source": [
        "# Create a sentiment analyzer\n",
        "sia = SentimentIntensityAnalyzer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "MzRl0eps_Akh"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "wKRcTMfu9iwB"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('/content/amazon_reviews_train.csv')\n",
        "def analyze_sentiment(text):\n",
        "    sentiment_scores = sia.polarity_scores(text)\n",
        "    return sentiment_scores\n",
        "\n",
        "#df['sentiment'] = df['review'].apply(analyze_sentiment)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "sX5EP-FUBZNx"
      },
      "outputs": [],
      "source": [
        "doc=df.review.to_list()\n",
        "tags=df.sentiment.to_list()\n",
        "seldocs=doc[1:10]\n",
        "seltags=tags[1:10]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T852QEhWHf-L",
        "outputId": "a238a22a-43d6-45d0-f400-3133793a2fd9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.8265 positive This is a confection that has been around a few centuries.  It is a light, pillowy citrus gelatin with nuts - in this case Filberts. And it is cut into tiny squares and then liberally coated with powdered sugar.  And it is a tiny mouthful of heaven.  Not too chewy, and very flavorful.  I highly recommend this yummy treat.  If you are familiar with the story of C.S. Lewis' \"The Lion, The Witch, and The Wardrobe\" - this is the treat that seduces Edmund into selling out his Brother and Sisters to the Witch.\n",
            "0.0 negative If you are looking for the secret ingredient in Robitussin I believe I have found it.  I got this in addition to the Root Beer Extract I ordered (which was good) and made some cherry soda.  The flavor is very medicinal.\n",
            "0.9468 positive Great taffy at a great price.  There was a wide assortment of yummy taffy.  Delivery was very quick.  If your a taffy lover, this is a deal.\n",
            "0.9346 positive This saltwater taffy had great flavors and was very soft and chewy.  Each candy was individually wrapped well.  None of the candies were stuck together, which did happen in the expensive version, Fralinger's.  Would highly recommend this candy!  I served it at a beach-themed party and everyone loved it!\n",
            "0.9487 positive This taffy is so good.  It is very soft and chewy.  The flavors are amazing.  I would definitely recommend you buying it.  Very satisfying!!\n",
            "0.9746 positive I don't know if it's the cactus or the tequila or just the unique combination of ingredients, but the flavour of this hot sauce makes it one of a kind!  We picked up a bottle once on a trip we were on and brought it back home with us and were totally blown away!  When we realized that we simply couldn't find it anywhere in our city we were bummed.<br /><br />Now, because of the magic of the internet, we have a case of the sauce and are ecstatic because of it.<br /><br />If you love hot sauce..I mean really love hot sauce, but don't want a sauce that tastelessly burns your throat, grab a bottle of Tequila Picante Gourmet de Inclan.  Just realize that once you taste it, you will never want to use any other sauce.<br /><br />Thank you for the personal, incredible service!\n",
            "0.296 negative My cats have been happily eating Felidae Platinum for more than two years. I just got a new bag and the shape of the food is different. They tried the new food when I first put it in their bowls and now the bowls sit full and the kitties will not touch the food. I've noticed similar reviews related to formula changes in the past. Unfortunately, I now need to find a new food that my cats will eat.\n",
            "0.9466 positive good flavor! these came securely packed... they were fresh and delicious! i love these Twizzlers!\n",
            "0.5719 positive My daughter loves twizzlers and this shipment of six pounds really hit the spot. It's exactly what you would expect...six packages of strawberry twizzlers.\n"
          ]
        }
      ],
      "source": [
        "for i in range(len(seldocs)):\n",
        "  s=sia.polarity_scores(seldocs[i])['compound']\n",
        "  print(s,  seltags[i], seldocs[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q3dTQV0G0ghK"
      },
      "source": [
        "#### Measure the accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1DuD823bx2z3",
        "outputId": "b8c6d3c8-4496-42d3-d5f4-3ca598f5d389"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 77.78%\n"
          ]
        }
      ],
      "source": [
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "import pandas as pd\n",
        "\n",
        "# Assuming you have loaded your data into the 'df' DataFrame\n",
        "\n",
        "# Initialize the SentimentIntensityAnalyzer\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Extract the text documents and true sentiment labels\n",
        "doc = df.review.to_list()\n",
        "tags = df.sentiment.to_list()\n",
        "seldocs = doc[1:10]\n",
        "seltags = tags[1:10]\n",
        "\n",
        "# Initialize variables to track correct predictions\n",
        "correct_predictions = 0\n",
        "total_predictions = len(seldocs)\n",
        "\n",
        "# Classify sentiments and measure accuracy\n",
        "for i in range(total_predictions):\n",
        "    compound_score = sia.polarity_scores(seldocs[i])['compound']\n",
        "    predicted_sentiment = 'positive' if compound_score >= 0 else 'negative'\n",
        "\n",
        "    # Check if the predicted sentiment matches the true sentiment label\n",
        "    if predicted_sentiment == seltags[i]:\n",
        "        correct_predictions += 1\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = correct_predictions / total_predictions\n",
        "\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xaU0IaeCybOk"
      },
      "source": [
        "# **3**. Preparação de dados e aplicação de um léxico de sentimentos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vPL4JQuFfi7b"
      },
      "source": [
        "The nltk.download('all') downloads all the datasets and packages available in the nltk library.\n",
        "\n",
        "\n",
        "*   This is necessary because some of the functions in the nltk library require specific datasets to be downloaded in order to work properly.\n",
        "*   By downloading all the datasets, the user can access all the functions in the library without having to worry about missing datasets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Preprocess the text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "mmudJJ6wFmRL"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Assuming 'df' is your DataFrame containing 'Sentiment' and 'Review' columns\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def normalize_text(text):\n",
        "    tokens = word_tokenize(text)  # Tokenize the text\n",
        "    tokens_lowered = [word.lower() for word in tokens]  # Lowercase each token\n",
        "    tokens_no_HTML_tag = [word for word in tokens_lowered if word not in ['br', '<br>', '<br />', '<br/>']]\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in tokens_filtered_no_quotes]  # Lemmatize the tokens\n",
        "\n",
        "    # Join the tokens back into a string\n",
        "\n",
        "    processed_text = ' '.join(lemmatized_tokens)\n",
        "\n",
        "    return processed_text\n",
        "\n",
        "# Apply text normalization to the 'Review' column\n",
        "\n",
        "df['Normalized_Review'] = df['review'].apply(normalize_text)\n",
        "\n",
        "# Display the updated DataFrame\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Apply Sentiment Lexicon"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### nrc_lexicon"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#data = pd.read_csv(\"../data/en/NCR-lexicon.csv\", encoding=\"utf-8\")\n",
        "data = pd.read_csv(\"https://raw.githubusercontent.com/fmmb/Text-Mining/main/data/NRC-lexicon.csv\", encoding=\"utf-8\")\n",
        "data.sample(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data.set_index(\"English\", inplace=True)\n",
        "lex1 = data[\"Positive\"] - data[\"Negative\"]\n",
        "lex1.sample(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "lex2 = lex1.to_dict()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def sentimento(texto):\n",
        "    soma = 0\n",
        "    for w in texto.split():\n",
        "        soma = soma + lex2.get(w, 0)\n",
        "    if soma >= 0:\n",
        "        return \"positive\"\n",
        "    else:\n",
        "        return \"negative\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to calculate the prediction for each row\n",
        "def calculate_prediction(row):\n",
        "    return sentimento(row['review'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Apply the function to each row to get the 'Prediction' column\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(classification_report(df['sentiment'], df['Prediction']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Preprocess the text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Assuming 'df' is your DataFrame containing 'Sentiment' and 'Review' columns\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def normalize_text(text):\n",
        "    tokens = word_tokenize(text)  # Tokenize the text\n",
        "    tokens_lowered = [word.lower() for word in tokens]  # Lowercase each token\n",
        "    tokens_no_HTML_tag = [word for word in tokens_lowered if word not in ['br', '<br>', '<br />', '<br/>']]\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in tokens_filtered_no_quotes]  # Lemmatize the tokens\n",
        "\n",
        "    # Join the tokens back into a string\n",
        "\n",
        "    processed_text = ' '.join(lemmatized_tokens)\n",
        "\n",
        "    return processed_text\n",
        "\n",
        "# Apply text normalization to the 'Review' column\n",
        "\n",
        "df['Normalized_Review'] = df['review'].apply(normalize_text)\n",
        "\n",
        "# Display the updated DataFrame\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to calculate the prediction for each row\n",
        "def calculate_prediction(row):\n",
        "    return sentimento(row['Normalized_Review'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Apply the function to each row to get the 'Prediction' column\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(classification_report(df['sentiment'], df['Prediction']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Negation Handling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "data = pd.read_csv(\"https://raw.githubusercontent.com/fmmb/Text-Mining/main/data/NRC-lexicon.csv\", encoding=\"utf-8\")\n",
        "data.set_index(\"English\", inplace=True)\n",
        "lex1 = data[\"Positive\"] - data[\"Negative\"]\n",
        "lex2 = lex1.to_dict()\n",
        "\n",
        "def sentimento(texto):\n",
        "    negation_terms = [\"not\", \"no\", \"never\"]  # Define negation terms\n",
        "    negate = False\n",
        "    soma = 0\n",
        "\n",
        "    for w in texto.split():\n",
        "        # Check for negation terms\n",
        "        if w in negation_terms:\n",
        "            negate = not negate\n",
        "        else:\n",
        "            if negate:\n",
        "                # Negate the sentiment of the word\n",
        "                soma = soma - lex2.get(w, 0)\n",
        "            else:\n",
        "                soma = soma + lex2.get(w, 0)\n",
        "    \n",
        "    if soma >= 0:\n",
        "        return \"positive\"\n",
        "    else:\n",
        "        return \"negative\"\n",
        "\n",
        "# Assuming 'df' is your DataFrame containing a 'Review' column\n",
        "# Replace this with your actual DataFrame\n",
        "# Example:\n",
        "# df = pd.read_csv('your_data.csv')\n",
        "\n",
        "# Function to calculate the prediction for each row\n",
        "def calculate_prediction(row):\n",
        "    return sentimento(row['Review'])\n",
        "\n",
        "# Apply the function to each row to get the 'Prediction' column\n",
        "df['Prediction'] = df.apply(calculate_prediction, axis=1)\n",
        "\n",
        "# Display the updated DataFrame\n",
        "print(df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qm9K8K-bFWt_"
      },
      "source": [
        "## Normalizing word formats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "-wPTDkIeFm--"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zhU9HTPPFda-"
      },
      "source": [
        "## Segmenting sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "AlctkUItx33f"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
