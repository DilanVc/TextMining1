{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQK54T76x9h4"
      },
      "source": [
        "## 2. Criação de uma baseline\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tbC3PDdnyJV6"
      },
      "source": [
        "### Vader Sentiment\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zRY6XUzAwp9A",
        "outputId": "b324c2b1-d3a8-4e94-edf4-2f5becb3e0e4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n"
          ]
        }
      ],
      "source": [
        "pip install nltk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zVSJ4NS69AF0"
      },
      "source": [
        "Import the necessary libraries and download the VADER lexicon."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AQrth5IQ8vcb",
        "outputId": "d1084410-ef6d-46f4-fd0b-84cbfebc031e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "\n",
        "# Download the VADER lexicon (if you haven't already)\n",
        "nltk.download('vader_lexicon')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1LaWG0vc9b9A"
      },
      "source": [
        "Create a SentimentIntensityAnalyzer object and use it to analyze text sentiment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "sLjJJWT585X9"
      },
      "outputs": [],
      "source": [
        "# Create a sentiment analyzer\n",
        "sia = SentimentIntensityAnalyzer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "MzRl0eps_Akh"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "wKRcTMfu9iwB"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('/content/amazon_reviews_train.csv')\n",
        "def analyze_sentiment(text):\n",
        "    sentiment_scores = sia.polarity_scores(text)\n",
        "    return sentiment_scores\n",
        "\n",
        "#df['sentiment'] = df['review'].apply(analyze_sentiment)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "sX5EP-FUBZNx"
      },
      "outputs": [],
      "source": [
        "doc=df.review.to_list()\n",
        "tags=df.sentiment.to_list()\n",
        "seldocs=doc[1:10]\n",
        "seltags=tags[1:10]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T852QEhWHf-L",
        "outputId": "a238a22a-43d6-45d0-f400-3133793a2fd9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.8265 positive This is a confection that has been around a few centuries.  It is a light, pillowy citrus gelatin with nuts - in this case Filberts. And it is cut into tiny squares and then liberally coated with powdered sugar.  And it is a tiny mouthful of heaven.  Not too chewy, and very flavorful.  I highly recommend this yummy treat.  If you are familiar with the story of C.S. Lewis' \"The Lion, The Witch, and The Wardrobe\" - this is the treat that seduces Edmund into selling out his Brother and Sisters to the Witch.\n",
            "0.0 negative If you are looking for the secret ingredient in Robitussin I believe I have found it.  I got this in addition to the Root Beer Extract I ordered (which was good) and made some cherry soda.  The flavor is very medicinal.\n",
            "0.9468 positive Great taffy at a great price.  There was a wide assortment of yummy taffy.  Delivery was very quick.  If your a taffy lover, this is a deal.\n",
            "0.9346 positive This saltwater taffy had great flavors and was very soft and chewy.  Each candy was individually wrapped well.  None of the candies were stuck together, which did happen in the expensive version, Fralinger's.  Would highly recommend this candy!  I served it at a beach-themed party and everyone loved it!\n",
            "0.9487 positive This taffy is so good.  It is very soft and chewy.  The flavors are amazing.  I would definitely recommend you buying it.  Very satisfying!!\n",
            "0.9746 positive I don't know if it's the cactus or the tequila or just the unique combination of ingredients, but the flavour of this hot sauce makes it one of a kind!  We picked up a bottle once on a trip we were on and brought it back home with us and were totally blown away!  When we realized that we simply couldn't find it anywhere in our city we were bummed.<br /><br />Now, because of the magic of the internet, we have a case of the sauce and are ecstatic because of it.<br /><br />If you love hot sauce..I mean really love hot sauce, but don't want a sauce that tastelessly burns your throat, grab a bottle of Tequila Picante Gourmet de Inclan.  Just realize that once you taste it, you will never want to use any other sauce.<br /><br />Thank you for the personal, incredible service!\n",
            "0.296 negative My cats have been happily eating Felidae Platinum for more than two years. I just got a new bag and the shape of the food is different. They tried the new food when I first put it in their bowls and now the bowls sit full and the kitties will not touch the food. I've noticed similar reviews related to formula changes in the past. Unfortunately, I now need to find a new food that my cats will eat.\n",
            "0.9466 positive good flavor! these came securely packed... they were fresh and delicious! i love these Twizzlers!\n",
            "0.5719 positive My daughter loves twizzlers and this shipment of six pounds really hit the spot. It's exactly what you would expect...six packages of strawberry twizzlers.\n"
          ]
        }
      ],
      "source": [
        "for i in range(len(seldocs)):\n",
        "  s=sia.polarity_scores(seldocs[i])['compound']\n",
        "  print(s,  seltags[i], seldocs[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q3dTQV0G0ghK"
      },
      "source": [
        "#### Measure the accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1DuD823bx2z3",
        "outputId": "b8c6d3c8-4496-42d3-d5f4-3ca598f5d389"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 77.78%\n"
          ]
        }
      ],
      "source": [
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "import pandas as pd\n",
        "\n",
        "# Assuming you have loaded your data into the 'df' DataFrame\n",
        "\n",
        "# Initialize the SentimentIntensityAnalyzer\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Extract the text documents and true sentiment labels\n",
        "doc = df.review.to_list()\n",
        "tags = df.sentiment.to_list()\n",
        "seldocs = doc[1:10]\n",
        "seltags = tags[1:10]\n",
        "\n",
        "# Initialize variables to track correct predictions\n",
        "correct_predictions = 0\n",
        "total_predictions = len(seldocs)\n",
        "\n",
        "# Classify sentiments and measure accuracy\n",
        "for i in range(total_predictions):\n",
        "    compound_score = sia.polarity_scores(seldocs[i])['compound']\n",
        "    predicted_sentiment = 'positive' if compound_score >= 0 else 'negative'\n",
        "\n",
        "    # Check if the predicted sentiment matches the true sentiment label\n",
        "    if predicted_sentiment == seltags[i]:\n",
        "        correct_predictions += 1\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = correct_predictions / total_predictions\n",
        "\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xaU0IaeCybOk"
      },
      "source": [
        "# **3**. Preparação de dados e aplicação de um léxico de sentimentos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vPL4JQuFfi7b"
      },
      "source": [
        "The nltk.download('all') downloads all the datasets and packages available in the nltk library.\n",
        "\n",
        "\n",
        "*   This is necessary because some of the functions in the nltk library require specific datasets to be downloaded in order to work properly.\n",
        "*   By downloading all the datasets, the user can access all the functions in the library without having to worry about missing datasets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Apply Sentiment Lexicon"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### nrc_lexicon"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#data = pd.read_csv(\"../data/en/NCR-lexicon.csv\", encoding=\"utf-8\")\n",
        "data = pd.read_csv(\"https://raw.githubusercontent.com/fmmb/Text-Mining/main/data/NRC-lexicon.csv\", encoding=\"utf-8\")\n",
        "data.sample(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data.set_index(\"English\", inplace=True)\n",
        "lex1 = data[\"Positive\"] - data[\"Negative\"]\n",
        "lex1.sample(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "lex2 = lex1.to_dict()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def sentimento(texto):\n",
        "    soma = 0\n",
        "    for w in texto.split():\n",
        "        soma = soma + lex2.get(w, 0)\n",
        "    if soma >= 0:\n",
        "        return \"positive\"\n",
        "    else:\n",
        "        return \"negative\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to calculate the prediction for each row\n",
        "def calculate_prediction(row):\n",
        "    return sentimento(row['review'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Apply the function to each row to get the 'Prediction' column\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(classification_report(df['sentiment'], df['Prediction']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Preprocess the text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Assuming 'df' is your DataFrame containing 'Sentiment' and 'Review' columns\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def normalize_text(text):\n",
        "    tokens = word_tokenize(text)  # Tokenize the text\n",
        "    tokens_lowered = [word.lower() for word in tokens]  # Lowercase each token\n",
        "    tokens_no_HTML_tag = [word for word in tokens_lowered if word not in ['br', '<br>', '<br />', '<br/>']]\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in tokens_filtered_no_quotes]  # Lemmatize the tokens\n",
        "\n",
        "    # Join the tokens back into a string\n",
        "\n",
        "    processed_text = ' '.join(lemmatized_tokens)\n",
        "\n",
        "    return processed_text\n",
        "\n",
        "# Apply text normalization to the 'Review' column\n",
        "\n",
        "df['Normalized_Review'] = df['review'].apply(normalize_text)\n",
        "\n",
        "# Display the updated DataFrame\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to calculate the prediction for each row\n",
        "def calculate_prediction(row):\n",
        "    return sentimento(row['Normalized_Review'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Apply the function to each row to get the 'Prediction' column\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(classification_report(df['sentiment'], df['Prediction']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Negation Handling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "data = pd.read_csv(\"https://raw.githubusercontent.com/fmmb/Text-Mining/main/data/NRC-lexicon.csv\", encoding=\"utf-8\")\n",
        "data.set_index(\"English\", inplace=True)\n",
        "lex1 = data[\"Positive\"] - data[\"Negative\"]\n",
        "lex2 = lex1.to_dict()\n",
        "\n",
        "negation_words = [\"not\", \"no\", \"never\"]  # Add more negation words as needed\n",
        "\n",
        "def sentimento(texto):\n",
        "    words = texto.split()\n",
        "    soma = 0\n",
        "    negation_multiplier = 1  # to handle negation\n",
        "\n",
        "    for i, w in enumerate(words):\n",
        "        if w in negation_words:\n",
        "            negation_multiplier = -1  # invert the sentiment\n",
        "        else:\n",
        "            soma = soma + (lex2.get(w, 0) * negation_multiplier)\n",
        "            negation_multiplier = 1  # reset the multiplier after handling the word\n",
        "\n",
        "    if soma >= 0:\n",
        "        return \"positive\"\n",
        "    else:\n",
        "        return \"negative\"\n",
        "\n",
        "# Function to calculate the prediction for each row\n",
        "def calculate_prediction(row):\n",
        "    return sentimento(row['review'])\n",
        "\n",
        "# Apply the function to each row to get the 'Prediction' column\n",
        "df['prediction_neg_handled'] = df.apply(calculate_prediction, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(classification_report(df['sentiment'], df['prediction_neg_handled']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qm9K8K-bFWt_"
      },
      "source": [
        "## 4. Treino de um modelo (aprendizagem automática)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "-wPTDkIeFm--"
      },
      "outputs": [],
      "source": [
        "from textblob import TextBlob\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import csv\n",
        "from nltk import word_tokenize, sent_tokenize\n",
        "from nltk.probability import FreqDist\n",
        "from textblob import TextBlob\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.read_csv(r'/content/amazon_reviews_train.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Group by 'Sentiment' and count\n",
        "sentiment_counts = df.groupby('sentiment').size()\n",
        "\n",
        "# Print the counts\n",
        "print(sentiment_counts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!jupyter notebook --NotebookApp.iopub_data_rate_limit=1000000000"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Training a model** (automatic learning)\n",
        "\n",
        "NLTK (Natural Language Toolkit) and Scikit-Learn are two powerful libraries in Python that are extensively used for text classification tasks. They offer a wide range of tools and algorithms to preprocess text data, extract features, and build machine learning models. Here's an overview of how they are utilized in text classification.\n",
        "We will test both and adjust the model to our data.\n",
        "Finnaly we will test our test set and measure the accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Trying with NLTK classify"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Preprocessing**\n",
        "\n",
        "1.   lowercasing, removing punctuation, stopword removal, and potentially stemming or lemmatization.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "df['review'] = df['review'].str.lower()\n",
        "df['review'] = df['review'].apply(lambda x: ''.join([char for char in x if char not in string.punctuation]))\n",
        "stop_words = set(stopwords.words('english'))\n",
        "df['review'] = df['review'].apply(lambda x: ' '.join([word for word in word_tokenize(x) if word not in stop_words]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The train_test_split function is a fundamental tool in machine learning. It serves a critical purpose by dividing a dataset into two distinct subsets: the training set and the testing set. The training set is used to train the model, allowing it to learn patterns and relationships within the data. Once trained, the model's performance is evaluated on the testing set, which contains data it has never seen before. This process mimics real-world scenarios where the model encounters new, unseen data. By assessing the model's performance on this independent set, we gain confidence in its ability to generalize well and make accurate predictions on future, unseen data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# splitting the data into training and testing sets, with 70% of the data used for testing and 30% used for training ( just to be more fast than all set)\n",
        "# the random state is set to 42 for reproducibility\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['review'], df['sentiment'], test_size=0.7, random_state=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Vectorize the Text**\n",
        "\n",
        "\n",
        "1.   Convert the text data into a numerical format that can be used by machine learning algorithms. Common techniques include TF-IDF vectorization or word embeddings like Word2Vec or GloVe.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=10000)\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Train the Binary Classifiers**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "#  binary_classifier is now a trained binary classifier capable of predicting the sentiment (positive or negative) of new, unseen text data\n",
        "\n",
        "binary_classifier = LogisticRegression()\n",
        "binary_classifier.fit(X_train_tfidf, y_train)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Evaluate the Binary Classifier**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "y_pred = binary_classifier.predict(X_test_tfidf)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "report = classification_report(y_test, y_pred)\n",
        "\n",
        "print(f'Accuracy: {accuracy}')\n",
        "print(f'Classification Report:\\n{report}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Apply the model you have built to the test set, evaluate the results obtained and compare them with the results obtained in the previous task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Test**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_test = pd.read_csv(r'/content/amazon_reviews_test.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Preprocess the test set\n",
        "df_test['review'] = df_test['review'].str.lower()\n",
        "df_test['review'] = df_test['review'].apply(lambda x: ''.join([char for char in x if char not in string.punctuation]))\n",
        "stop_words = set(stopwords.words('english'))\n",
        "df_test['review'] = df_test['review'].apply(lambda x: ' '.join([word for word in word_tokenize(x) if word not in stop_words]))\n",
        "\n",
        "# The split data is not applicable\n",
        "\n",
        "# Vectorize the test set\n",
        "X_test = df_test['review']\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "# Apply the binary classifier on the test set\n",
        "y_pred = binary_classifier.predict(X_test_tfidf)\n",
        "\n",
        "# Evaluate the performance\n",
        "accuracy = accuracy_score(df_test['sentiment'], y_pred)\n",
        "report = classification_report(df_test['sentiment'], y_pred)\n",
        "\n",
        "print(f'Accuracy: {accuracy}')\n",
        "print(f'Classification Report:\\n{report}')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Trying with scikit-learn classify\n",
        "\n",
        "1.   With Logistic Regression\n",
        "2.   With Naive Bayes\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Preprocessing the data and Split the data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df['review'] = df['review'].str.lower()\n",
        "df['review'] = df['review'].apply(lambda x: ''.join([char for char in x if char not in string.punctuation]))\n",
        "stop_words = set(stopwords.words('english'))\n",
        "df['review'] = df['review'].apply(lambda x: ' '.join([word for word in word_tokenize(x) if word not in stop_words]))\n",
        "\n",
        "# Step 2: Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['review'], df['sentiment'], test_size=0.3, random_state=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vectorize the text data with TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Logistic Regression\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=10000)\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Train with Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "binary_classifier = LogisticRegression()\n",
        "binary_classifier.fit(X_train_tfidf, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_pred = binary_classifier.predict(X_test_tfidf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "report = classification_report(y_test, y_pred)\n",
        "\n",
        "print(f'Accuracy: {accuracy}')\n",
        "print(f'Classification Report:\\n{report}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "With Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Naive Bayes\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Preprocess and Split the data (the same before)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df['review'] = df['review'].str.lower()\n",
        "df['review'] = df['review'].apply(lambda x: ''.join([char for char in x if char not in string.punctuation]))\n",
        "stop_words = set(stopwords.words('english'))\n",
        "df['review'] = df['review'].apply(lambda x: ' '.join([word for word in word_tokenize(x) if word not in stop_words]))\n",
        "\n",
        "# Step 2: Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['review'], df['sentiment'], test_size=0.3, random_state=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vectorize the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tfidf_vectorizer = TfidfVectorizer(max_features=10000)\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Train the Classifier with Multinomial Naive Bayes**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "naive_bayes_classifier = MultinomialNB()\n",
        "naive_bayes_classifier.fit(X_train_tfidf, y_train)\n",
        "\n",
        "#Predict sentiments on the test set\n",
        "y_pred = naive_bayes_classifier.predict(X_test_tfidf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "classifier = MultinomialNB()\n",
        "classifier.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Classify text and measure accuracy**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "report = classification_report(y_test, y_pred)\n",
        "\n",
        "print(f'Accuracy: {accuracy}')\n",
        "print(f'Classification Report:\\n{report}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Individual Predictions - Just to understand one by one how is classify"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for idx, (review, true_label, pred_label) in enumerate(zip(df['review'][y_test.index], y_test, y_pred)):\n",
        "    print(f\"Text {idx+1}:\")\n",
        "    print(f\"Review: {review}\")\n",
        "    print(f\"True Label: {true_label}\")\n",
        "    print(f\"Predicted Label: {pred_label}\")\n",
        "    print(f\"Accuracy: {'Correct' if true_label == pred_label else 'Incorrect'}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Therefore, for the test data, the logistic regression fits better, resulting in better accuracy**\n",
        "\n",
        "So, let's test with test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 1: Preprocess the data\n",
        "df_test['review'] = df_test['review'].str.lower()\n",
        "df_test['review'] = df_test['review'].apply(lambda x: ''.join([char for char in x if char not in string.punctuation]))\n",
        "stop_words = set(stopwords.words('english'))\n",
        "df_test['review'] = df_test['review'].apply(lambda x: ' '.join([word for word in word_tokenize(x) if word not in stop_words]))\n",
        "\n",
        "# Step 2: Vectorize the text data using TF-IDF\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=10000)\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(df['review'])\n",
        "X_test_tfidf = tfidf_vectorizer.transform(df_test['review'])\n",
        "\n",
        "# Step 3: Train the binary classifier (Logistic Regression)\n",
        "binary_classifier = LogisticRegression()\n",
        "binary_classifier.fit(X_train_tfidf, df['sentiment'])\n",
        "\n",
        "# Step 4: Apply the trained binary classifier on the test set\n",
        "y_pred = binary_classifier.predict(X_test_tfidf)\n",
        "\n",
        "# Step 5: Evaluate the performance\n",
        "accuracy = accuracy_score(df_test['sentiment'], y_pred)\n",
        "report = classification_report(df_test['sentiment'], y_pred)\n",
        "\n",
        "print(f'Accuracy: {accuracy}')\n",
        "print(f'Classification Report:\\n{report}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 5. Utilização de transformadores\n",
        "\n",
        "1.  as a first step, you can carry out simple experiments, using already defined pipelines, applying one or more existing models;\n",
        "2. as a second step, use your data to finetune the pre-trained model and thus achieve even better results.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sentiment_pipeline = pipeline(\"text-classification\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# This command takes a lot of time run\n",
        "\n",
        "# Apply sentiment analysis using the pipeline\n",
        "#results = sentiment_pipeline(df['review'].tolist())\n",
        "\n",
        "# Print out the sentiment predictions\n",
        "#for idx, result in enumerate(results):\n",
        "#    print(f\"Text {idx+1}: {result['label']} (confidence: {result['score']:.4f})\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Preprocessing\n",
        "df_test['review'] = df_test['review'].str.lower()\n",
        "df_test['review'] = df_test['review'].apply(lambda x: ''.join([char for char in x if char not in string.punctuation]))\n",
        "stop_words = set(stopwords.words('english'))\n",
        "df_test['review'] = df_test['review'].apply(lambda x: ' '.join([word for word in word_tokenize(x) if word not in stop_words]))\n",
        "\n",
        "# Sample a subset of the test data\n",
        "subset_df_test = df_test.sample(n=100, random_state=42)\n",
        "\n",
        "# Apply sentiment analysis using the pipeline on the subset\n",
        "results = sentiment_pipeline(subset_df_test['review'].tolist())\n",
        "\n",
        "# Extract predicted sentiments and convert to lowercase\n",
        "predicted_labels = [result['label'].lower() for result in results]\n",
        "\n",
        "# Map sentiment labels to binary labels\n",
        "binary_labels = {'positive': 1, 'negative': 0}\n",
        "y_pred_binary = [binary_labels[label] for label in predicted_labels]\n",
        "\n",
        "# Vectorize with TF-IDF\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=10000)\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(df['review'])\n",
        "X_test_tfidf = tfidf_vectorizer.transform(subset_df_test['review'])\n",
        "\n",
        "# Train the binary classifier (Logistic Regression)\n",
        "binary_classifier = LogisticRegression()\n",
        "binary_classifier.fit(X_train_tfidf, df['sentiment'])\n",
        "\n",
        "# Apply the trained binary classifier on the test set\n",
        "y_pred = binary_classifier.predict(X_test_tfidf)\n",
        "\n",
        "# Measure the accuracy\n",
        "accuracy = accuracy_score(subset_df_test['sentiment'], y_pred)\n",
        "report = classification_report(subset_df_test['sentiment'], y_pred)\n",
        "\n",
        "print(f'Accuracy: {accuracy}')\n",
        "print(f'Classification Report:\\n{report}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now trying with other pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sentiment_pipeline_distilbert = pipeline(\"sentiment-analysis\", model=\"distilbert-base-uncased\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Preprocessing\n",
        "df['review'] = df['review'].str.lower()\n",
        "df['review'] = df['review'].apply(lambda x: ''.join([char for char in x if char not in string.punctuation]))\n",
        "stop_words = set(stopwords.words('english'))\n",
        "df['review'] = df['review'].apply(lambda x: ' '.join([word for word in word_tokenize(x) if word not in stop_words]))\n",
        "\n",
        "# Sample a subset of the test data\n",
        "subset_df_test = df_test.sample(n=100, random_state=42)\n",
        "\n",
        "# Apply sentiment analysis using the pipeline\n",
        "results_distilbert = sentiment_pipeline_distilbert(subset_df_test['review'].tolist())\n",
        "\n",
        "# Extract predicted sentiments\n",
        "predicted_labels_distilbert = [result['label'] for result in results_distilbert]\n",
        "\n",
        "# Map sentiment labels to binary labels for DistilBERT predictions\n",
        "binary_labels_distilbert = {'LABEL_1': 1, 'LABEL_0': 0}\n",
        "y_pred_binary_distilbert = [binary_labels_distilbert[label] for label in predicted_labels_distilbert]\n",
        "\n",
        "# Convert sentiment labels to binary labels for logistic regression\n",
        "binary_labels_logistic = {'positive': 1, 'negative': 0}\n",
        "y_pred_binary_logistic = [binary_labels_logistic[label] for label in y_pred]\n",
        "\n",
        "\n",
        "# Evaluate the performance\n",
        "accuracy = accuracy_score(y_pred_binary_distilbert, y_pred_binary_logistic)\n",
        "report = classification_report(y_pred_binary_distilbert, y_pred_binary_logistic)\n",
        "\n",
        "\n",
        "print(f'Accuracy: {accuracy}')\n",
        "print(f'Classification Report:\\n{report}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "https://www.kaggle.com/code/pritishmishra/text-classification-with-distilbert-92-accuracy\n",
        "\n",
        "https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from transformers import pipeline, DistilBertTokenizer, DistilBertForSequenceClassification\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "\n",
        "# Assuming you have already loaded your df_test DataFrame\n",
        "\n",
        "# Step 1: Preprocess the data\n",
        "df_test['review'] = df_test['review'].str.lower()\n",
        "df_test['review'] = df_test['review'].apply(lambda x: ''.join([char for char in x if char not in string.punctuation]))\n",
        "stop_words = set(stopwords.words('english'))\n",
        "df_test['review'] = df_test['review'].apply(lambda x: ' '.join([word for word in word_tokenize(x) if word not in stop_words]))\n",
        "\n",
        "# Step 2: Sample a smaller subset of the data to speed up processing\n",
        "subset_df_test = df_test.sample(n=1000, random_state=42)\n",
        "\n",
        "# Step 3: Vectorize the text data using TF-IDF\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=10000)\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(df['review'])\n",
        "X_test_tfidf = tfidf_vectorizer.transform(subset_df_test['review'])\n",
        "\n",
        "# Step 4: Define a pipeline for fill-mask task\n",
        "fill_mask_pipeline = pipeline('fill-mask', model='distilbert-base-uncased')\n",
        "\n",
        "# Step 5: Apply the pipeline to generate masked sentences\n",
        "masked_sentences = []\n",
        "for text in subset_df_test['review']:\n",
        "    mask_position = len(text.split()) // 2\n",
        "    masked_text = ' '.join([f'[MASK]' if i == mask_position else word for i, word in enumerate(text.split())])\n",
        "    masked_sentences.append(masked_text)\n",
        "\n",
        "# Step 6: Use the pipeline to predict the missing word's sentiment\n",
        "predicted_sentiments = []\n",
        "for masked_sentence in masked_sentences:\n",
        "    results = fill_mask_pipeline(masked_sentence)\n",
        "    predicted_word = results[0]['token_str']\n",
        "    predicted_sentiments.append(predicted_word)\n",
        "\n",
        "# Step 7: Apply logistic regression to classify the predicted sentiments\n",
        "binary_classifier = LogisticRegression()\n",
        "binary_classifier.fit(X_train_tfidf, df['sentiment'])\n",
        "\n",
        "# Step 8: Vectorize the predicted sentiments using TF-IDF\n",
        "X_predicted_tfidf = tfidf_vectorizer.transform(predicted_sentiments)\n",
        "\n",
        "# Step 9: Classify the predicted sentiments\n",
        "y_pred = binary_classifier.predict(X_predicted_tfidf)\n",
        "\n",
        "# Step 10: Measure accuracy\n",
        "accuracy = accuracy_score(subset_df_test['sentiment'], y_pred)\n",
        "report = classification_report(subset_df_test['sentiment'], y_pred)\n",
        "\n",
        "print(f'Accuracy: {accuracy}')\n",
        "print(f'Classification Report:\\n{report}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 1: Preprocess the data (assuming df_test is your test DataFrame)\n",
        "df_test['review'] = df_test['review'].str.lower()\n",
        "df_test['review'] = df_test['review'].apply(lambda x: ''.join([char for char in x if char not in string.punctuation]))\n",
        "stop_words = set(stopwords.words('english'))\n",
        "df_test['review'] = df_test['review'].apply(lambda x: ' '.join([word for word in word_tokenize(x) if word not in stop_words]))\n",
        "\n",
        "# Step 2: Use a Transformer pipeline for sentiment analysis\n",
        "sentiment_pipeline = pipeline(\"sentiment-analysis\", model=\"distilbert-base-uncased\")\n",
        "\n",
        "# Step 3: Apply sentiment analysis using the pipeline on a subset\n",
        "results = sentiment_pipeline(df_test['review'].tolist())\n",
        "\n",
        "# Extract predicted sentiments and convert to lowercase\n",
        "predicted_labels = [result['label'].lower() for result in results]\n",
        "\n",
        "# Map sentiment labels to binary labels\n",
        "# Assuming the labels from the pipeline are in the format 'LABEL_X'\n",
        "binary_labels = {'LABEL_1': 1, 'LABEL_0': 0}\n",
        "predicted_labels = [label.upper() for label in predicted_labels]\n",
        "y_pred_binary = [binary_labels[label] for label in predicted_labels]\n",
        "\n",
        "\n",
        "# Step 4: Vectorize the text data using TF-IDF\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=50000)\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(df['review'])\n",
        "X_test_tfidf = tfidf_vectorizer.transform(df_test['review'])\n",
        "\n",
        "# Step 5: Train the binary classifier (Logistic Regression)\n",
        "binary_classifier = LogisticRegression(max_iter=10000)\n",
        "binary_classifier.fit(X_train_tfidf, df['sentiment'])\n",
        "\n",
        "# Step 6: Apply the trained binary classifier on the test set\n",
        "y_pred = binary_classifier.predict(X_test_tfidf)\n",
        "\n",
        "# Step 7: Measure the accuracy\n",
        "accuracy = accuracy_score(df_test['sentiment'], y_pred)\n",
        "report = classification_report(df_test['sentiment'], y_pred)\n",
        "\n",
        "print(f'Accuracy: {accuracy}')\n",
        "print(f'Classification Report:\\n{report}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Map sentiment labels to binary labels\n",
        "# Assuming the labels from the pipeline are in the format 'LABEL_X'\n",
        "binary_labels = {'LABEL_1': 1, 'LABEL_0': 0}\n",
        "predicted_labels = [label.upper() for label in predicted_labels]\n",
        "y_pred_binary = [binary_labels[label] for label in predicted_labels]\n",
        "\n",
        "\n",
        "# Step 4: Vectorize the text data using TF-IDF\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=50000)\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(df['review'])\n",
        "X_test_tfidf = tfidf_vectorizer.transform(df_test['review'])\n",
        "\n",
        "# Step 5: Train the binary classifier (Logistic Regression)\n",
        "binary_classifier = LogisticRegression(max_iter=10000)\n",
        "binary_classifier.fit(X_train_tfidf, df['sentiment'])\n",
        "\n",
        "# Step 6: Apply the trained binary classifier on the test set\n",
        "y_pred = binary_classifier.predict(X_test_tfidf)\n",
        "\n",
        "# Step 7: Measure the accuracy\n",
        "accuracy = accuracy_score(df_test['sentiment'], y_pred)\n",
        "report = classification_report(df_test['sentiment'], y_pred)\n",
        "\n",
        "print(f'Accuracy: {accuracy}')\n",
        "print(f'Classification Report:\\n{report}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Fine-tuning involves training the pre-trained model on your specific task or dataset. You can use a sentiment analysis dataset and fine-tune a transformer-based model (like BERT or DistilBERT) on it.\n",
        "This will allow the model to adapt to the specific nuances and characteristics of your sentiment analysis task.\n",
        "\n",
        "Experiment with Different Models:\n",
        "\n",
        "Try using different pre-trained models and architectures. For example, you can experiment with BERT, RoBERTa, XLNet, etc., and see which one performs better for your specific task.\n",
        "\n",
        "Data Augmentation:\n",
        "\n",
        "You can generate additional training data by applying techniques like paraphrasing, back-translation, or using synonyms. This can help in exposing the model to a wider range of sentence structures and sentiments.\n",
        "\n",
        "Ensemble Methods:\n",
        "\n",
        "Combine predictions from multiple models to improve accuracy. You can use techniques like averaging, stacking, or even using a voting classifier.\n",
        "\n",
        "Balancing the Dataset:\n",
        "\n",
        "If your dataset is imbalanced (i.e., it has significantly more samples of one class than the other), consider techniques like oversampling, undersampling, or using synthetic data generation methods.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fine tunning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_train = pd.read_csv(r'/content/amazon_reviews_train.csv')\n",
        "df_test = pd.read_csv(r'/content/amazon_reviews_test.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import Dataset, DatasetDict\n",
        "import pandas as pd\n",
        "\n",
        "# Convert the pandas DataFrames to datasets\n",
        "train_dataset = Dataset.from_pandas(df_train)\n",
        "test_dataset = Dataset.from_pandas(df_test)\n",
        "\n",
        "# Create a DatasetDict containing train and test splits\n",
        "dataset_dict = DatasetDict({'train': train_dataset, 'test': test_dataset})\n",
        "\n",
        "print(dataset_dict)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "small_train_dataset = dataset_dict[\"train\"].shuffle(seed=42).select([i for i in list(range(3000))])\n",
        "small_test_dataset = dataset_dict[\"test\"].shuffle(seed=42).select([i for i in list(range(300))])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def preprocess_function(examples):\n",
        "   return tokenizer(examples[\"review\"], truncation=True)\n",
        "\n",
        "tokenized_train = small_train_dataset.map(preprocess_function, batched=True)\n",
        "tokenized_test = small_test_dataset.map(preprocess_function, batched=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import DataCollatorWithPadding\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import DataCollatorWithPadding\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from datasets import load_metric\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "   load_accuracy = load_metric(\"accuracy\")\n",
        "   load_f1 = load_metric(\"f1\")\n",
        "\n",
        "   logits, labels = eval_pred\n",
        "   predictions = np.argmax(logits, axis=-1)\n",
        "   accuracy = load_accuracy.compute(predictions=predictions, references=labels)[\"accuracy\"]\n",
        "   f1 = load_f1.compute(predictions=predictions, references=labels)[\"f1\"]\n",
        "   return {\"accuracy\": accuracy, \"f1\": f1}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pip install torch==2.0.0\n",
        "!pip install accelerate>=0.20.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import transformers\n",
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "repo_name = \"dilanveracruz/textMining\"\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "   output_dir=repo_name,\n",
        "   learning_rate=2e-5,\n",
        "   per_device_train_batch_size=16,\n",
        "   per_device_eval_batch_size=16,\n",
        "   num_train_epochs=2,\n",
        "   weight_decay=0.01,\n",
        "   save_strategy=\"epoch\",\n",
        "   push_to_hub=True,\n",
        "   prediction_loss_only=False\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "   model=model,\n",
        "   args=training_args,\n",
        "   train_dataset=tokenized_train,\n",
        "   eval_dataset=tokenized_test,\n",
        "   tokenizer=tokenizer,\n",
        "   data_collator=data_collator,\n",
        "   compute_metrics=compute_metrics,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "trainer.evaluate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install torch\n",
        "!pip install scikit-learn\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load your data (assuming you have a DataFrame 'df' with 'review' and 'sentiment' columns)\n",
        "# df = pd.read_csv('your_data.csv')\n",
        "\n",
        "# Preprocess data (if necessary)\n",
        "df['review'] = df['review'].str.lower()\n",
        "df['review'] = df['review'].str.replace('[^\\w\\s]', '')  # Remove punctuation\n",
        "df['review'] = df['review'].str.replace('\\d+', '')  # Remove numbers\n",
        "df['review'] = df['review'].str.strip()  # Remove leading/trailing spaces\n",
        "\n",
        "# Split data into train and test sets\n",
        "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "\n",
        "# Save train and test data to separate CSV files\n",
        "train_df.to_csv('train_data.csv', index=False)\n",
        "test_df.to_csv('test_data.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zhU9HTPPFda-"
      },
      "source": [
        "## Segmenting sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "AlctkUItx33f"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
